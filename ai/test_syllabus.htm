<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Minor in AI Syllabus (Modules A–C)</title>
  <!-- Raleway font from Google Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link
    href="https://fonts.googleapis.com/css2?family=Raleway:wght@300;400;500;600;700&display=swap"
    rel="stylesheet"
  />
  <style>
    body {
      font-family: 'Raleway', sans-serif;
      line-height: 1.6;
      margin: 2rem;
      color: #333;
    }
    h1 {
      font-size: 2rem;
      margin-bottom: 1rem;
    }
    h2 {
      font-size: 1.5rem;
      margin-top: 2rem;
      margin-bottom: 1rem;
      border-bottom: 2px solid #ddd;
      padding-bottom: 0.25rem;
    }
    h3 {
      font-size: 1.25rem;
      margin-top: 1.25rem;
      margin-bottom: 0.5rem;
    }
    ul {
      margin-left: 1.5rem;
      margin-bottom: 1rem;
    }
    li {
      margin-bottom: 0.25rem;
    }
  </style>
</head>
<body>
  <h1>Minor in AI Syllabus (Modules A–C)</h1>
  <p>
    This document outlines the syllabus for a Minor in Artificial Intelligence,
    covering three modules (A, B, and C). Each module includes foundational and
    advanced topics designed to build knowledge sequentially.
  </p>

  <!-- Module A -->
  <h2>Module A: Foundations</h2>

  <h3>Python Programming with Prompting</h3>
  <ul>
    <li>Basics of Python</li>
    <li>Variables and data types</li>
    <li>Control structures: loops and conditions</li>
    <li>Functions</li>
    <li>Error and exception handling</li>
    <li>File operations</li>
    <li>Data structures: lists, dictionaries, sets, tuples</li>
    <li>Sorting algorithms</li>
    <li>Algorithm efficiency</li>
  </ul>

  <h3>Statistics for Data Science</h3>
  <ul>
    <li>Basics of statistics</li>
    <li>Descriptive statistics
      <ul>
        <li>Mean, Median, Mode</li>
        <li>Variance, Standard Deviation</li>
        <li>Range, Interquartile Range (IQR)</li>
        <li>Skewness, Kurtosis</li>
      </ul>
    </li>
    <li>Frequency distributions and histograms</li>
    <li>Inferential statistics
      <ul>
        <li>Population vs. Sample</li>
        <li>Hypothesis Testing</li>
        <li>P-test (P-value, significance level, interpretation)</li>
        <li>Confidence Intervals</li>
        <li>Critical value</li>
        <li>Z-test, T-test</li>
      </ul>
    </li>
  </ul>

  <h3>Mathematics for Data Science</h3>
  <ul>
    <li>Linear Algebra
      <ul>
        <li>Vectors</li>
        <li>Matrices</li>
        <li>Matrix operations: addition, multiplication, transpose, inverse</li>
        <li>Dot product and cross product</li>
        <li>Eigenvalues and Eigenvectors</li>
      </ul>
    </li>
    <li>Calculus
      <ul>
        <li>Derivatives and gradients</li>
        <li>Partial derivatives</li>
        <li>Chain rule</li>
        <li>Optimization: minima and maxima</li>
      </ul>
    </li>
    <li>Probability
      <ul>
        <li>Sample space and events</li>
        <li>Conditional probability</li>
        <li>Bayes’ theorem</li>
        <li>Probability distributions: binomial, normal, etc.</li>
      </ul>
    </li>
    <li>Set theory and logic</li>
  </ul>

  <!-- Module B -->
  <h2>Module B: Core Machine Learning & Deep Learning</h2>

  <h3>Machine Learning Concepts</h3>
  <ul>
    <li>Regression
      <ul>
        <li>Linear regression</li>
        <li>Polynomial regression</li>
      </ul>
    </li>
    <li>Dimensionality reduction</li>
    <li>PageRank</li>
    <li>Foundations of perception</li>
    <li>Enhancing the perceptron with learning algorithms</li>
    <li>Neural networks</li>
  </ul>

  <h3>Convolutional Neural Networks (CNNs)</h3>
  <ul>
    <li>Convolution operation</li>
    <li>Pooling</li>
    <li>Activation functions
      <ul>
        <li>Sigmoid</li>
        <li>Tanh</li>
        <li>ReLU</li>
        <li>LeakyReLU</li>
        <li>Softmax</li>
        <li>Swish</li>
      </ul>
    </li>
    <li>Architectures
      <ul>
        <li>ImageNet 1k Data</li>
        <li>LeNet-5</li>
        <li>AlexNet</li>
        <li>VGGNet</li>
      </ul>
    </li>
  </ul>

  <h3>Support Vector Machine (SVM)</h3>
  <ul>
    <li>Concepts and Mathematical formulation</li>
    <li>Kernel functions</li>
    <li>Application examples</li>
  </ul>

  <h3>Supervised Learning</h3>
  <ul>
    <li>Introduction and types of supervised learning</li>
    <li>Evaluation metrics
      <ul>
        <li>Accuracy</li>
        <li>Precision</li>
        <li>Recall</li>
        <li>F1-score</li>
        <li>Receiver Operating Characteristic (ROC) Curve</li>
      </ul>
    </li>
  </ul>

  <h3>Unsupervised Learning</h3>
  <ul>
    <li>K-means clustering</li>
    <li>Evaluation metrics for clustering</li>
  </ul>

  <!-- Module C -->
  <h2>Module C: Advanced AI Topics</h2>

  <h3>Sequence Models & Natural Language Processing</h3>
  <ul>
    <li>Sequence Models
      <ul>
        <li>Importance of sequence models</li>
        <li>Autoregressive models</li>
        <li>Markov Model</li>
      </ul>
    </li>
    <li>Text Processing
      <ul>
        <li>Tokenization</li>
        <li>Vocabulary building</li>
        <li>n-grams</li>
        <li>Perplexity</li>
      </ul>
    </li>
    <li>Recurrent Neural Networks (RNNs) & Training
      <ul>
        <li>RNN architecture</li>
        <li>Backpropagation Through Time (BPTT)</li>
        <li>Vanishing gradients</li>
        <li>GRUs (Gated Recurrent Units)</li>
        <li>LSTMs (Long Short-Term Memory)</li>
      </ul>
    </li>
    <li>Word Embeddings
      <ul>
        <li>Word similarity</li>
        <li>GloVe (Global Vectors for Word Representation)</li>
        <li>Applications in NLP</li>
      </ul>
    </li>
  </ul>

  <h3>Advanced Architectures & Large Language Models</h3>
  <ul>
    <li>Deep RNNs</li>
    <li>Bi-Directional RNNs</li>
    <li>Transformers</li>
    <li>Practical Applications
      <ul>
        <li>Sentiment classification</li>
        <li>Next-word prediction</li>
        <li>Machine translation</li>
        <li>Speech recognition</li>
      </ul>
    </li>
    <li>Attention Mechanisms
      <ul>
        <li>Self-Attention</li>
        <li>Multi-head Attention</li>
        <li>Encoder-Decoder models</li>
      </ul>
    </li>
    <li>Transformers and Attention Mechanisms (further details)</li>
    <li>Training Large Language Models (LLMs)</li>
    <li>Prompt Engineering</li>
    <li>Applications and Limitations of LLMs</li>
    <li>Evaluation Techniques for LLMs</li>
    <li>Efficiency Techniques for LLM training and inference</li>
    <li>Retrieval-Augmented Generation (RAG)
      <ul>
        <li>Building RAG Systems</li>
        <li>RAG Applications and Challenges</li>
      </ul>
    </li>
  </ul>

  <h3>Semi-Supervised Learning</h3>
  <ul>
    <li>Introduction to Semi-Supervised Learning (SSL)</li>
    <li>SSL Assumptions and Fundamentals</li>
    <li>Ladder Networks and Π-Models</li>
    <li>Proxy Label Methods
      <ul>
        <li>Self-Training</li>
        <li>Co-Training</li>
      </ul>
    </li>
    <li>Variational Autoencoders (VAEs) for SSL</li>
    <li>Graph-Based SSL: Graph Neural Networks (GNNs) and Label Propagation</li>
  </ul>

  <h3>Reinforcement Learning</h3>
  <ul>
    <li>Introduction to Reinforcement Learning (RL)</li>
    <li>Exploration vs. Exploitation: The Multi-Armed Bandit Problem</li>
    <li>Markov Decision Processes (MDPs)</li>
    <li>Solving MDPs with Dynamic Programming</li>
    <li>Monte Carlo Methods in Reinforcement Learning</li>
    <li>Temporal Difference Learning</li>
    <li>Deep Reinforcement Learning</li>
  </ul>
</body>
</html>
